{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiNpFfj9I40G"
      },
      "source": [
        "# __Image Classification Using CNN__\n",
        "\n",
        "Let's see a working example of training a convolutional neural network (CNN) on a dataset of flower images."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps to Be Followed:\n",
        "\n",
        "1. Importing the necessary libraries and dataset\n",
        "2. Counting and retrieving the images\n",
        "3. Creating a training dataset\n",
        "4. Creating a validation dataset\n",
        "5. Visualizing a subset of images from the training dataset\n",
        "6. Preprocessing and normalizing the training dataset\n",
        "7. Creating a convolutional neural network model with data augmentation\n",
        "8. Summarizing and compiling the model\n",
        "9. Training the model\n",
        "10. Visualizing the result\n",
        "11. Predicting the class of a given image"
      ],
      "metadata": {
        "id": "IaUqF90SEsW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Importing the Necessary Libraries and Dataset\n",
        "- Import the required libraries\n",
        "- Import the given dataset using given link"
      ],
      "metadata": {
        "id": "cg3LB4H0E86T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xUn3p2qiI40I"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lhAaJoppI40K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96300385-a9ff-42e1-efeb-eccf53a02ff8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\n",
            "228813984/228813984 [==============================] - 8s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import pathlib\n",
        "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
        "data_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\n",
        "data_dir = pathlib.Path(data_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Counting and Retrieving the Images\n",
        "- Count the number of images in the directory specified by **data_dir** and print the count\n",
        "- Retrieve the file paths of the images in the **roses** subdirectory and display the first two images\n",
        "- Retrieve the file paths of the images in the __tulips__ subdirectory and display the first image"
      ],
      "metadata": {
        "id": "qWrqGcAiJEYw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y43j23jI40K"
      },
      "outputs": [],
      "source": [
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFgtZ4MgI40L"
      },
      "outputs": [],
      "source": [
        "roses = list(data_dir.glob('roses/*'))\n",
        "PIL.Image.open(str(roses[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLtIHc3GI40L"
      },
      "outputs": [],
      "source": [
        "PIL.Image.open(str(roses[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R07HummwI40M"
      },
      "outputs": [],
      "source": [
        "tulips = list(data_dir.glob('tulips/*'))\n",
        "PIL.Image.open(str(tulips[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Creating a Training Dataset\n",
        "- Set the batch size, image height, and image width variables\n",
        "- Create a training dataset using **tf.keras.utils.image_dataset_from_directory()** function, passing the following parameters:\n",
        "  - __data_dir:__ The directory containing the image dataset\n",
        "  - **validation_split**: The fraction of data to reserve for validation\n",
        "  - **subset**: Specify the subset of the dataset to use (in this case, training)\n",
        "  - **seed**: Random seed for shuffling the data\n",
        "  - **image_size**: The desired size for the images in the dataset\n",
        "  - **batch_size**: The number of samples per batch\n"
      ],
      "metadata": {
        "id": "SS0YAxJ1RrA8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-zIt-KJI40M"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180\n",
        "\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The **train_ds** object representing the training dataset."
      ],
      "metadata": {
        "id": "9kc3AnzuaYQN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Creating a Validation Dataset\n",
        "- Create a validation dataset using **tf.keras.utils.image_dataset_from_directory()** function, passing the following parameters:\n",
        "  - __data_dir:__ The directory containing the image dataset\n",
        "  - **validation_split**: The fraction of data to reserve for validation\n",
        "  - **subset**: Specify the subset of the dataset to use (in this case, validation)\n",
        "  - **seed**: Random seed for shuffling the data\n",
        "  - **image_size**: The desired size for the images in the dataset\n",
        "  - **batch_size**: The number of samples per batch"
      ],
      "metadata": {
        "id": "q1oCPJfDbJ0n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCVAh6tKI40M"
      },
      "outputs": [],
      "source": [
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The __val_ds__ object representing the validation dataset."
      ],
      "metadata": {
        "id": "apOKN9rMbk2W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Visualizing a Subset of Images from the Training Dataset\n",
        "- Obtain the class names from the train_ds dataset using the class_names attribute\n",
        "- Print the **class_names** to display the list of class labels\n",
        "- Import the **matplotlib.pyplot** module for visualization purposes\n",
        "- Create a figure with a size of 10x10 using **plt.figure(figsize=(10, 10))**\n",
        "- Iterate over the first batch of images and labels in the **train_ds** dataset using **train_ds.take(1)**\n",
        "- For each image in the batch (up to 9 images), create a subplot using **plt.subplot(3, 3, i + 1)**\n",
        "- Display the image using **plt.imshow(images[i].numpy().astype(\"uint8\"))**\n",
        "- Set the title of the subplot to the corresponding class name using **plt.title(class_names[labels[i]])**\n",
        "- Disable the axis labels for the subplot using __plt.axis(\"off\")__"
      ],
      "metadata": {
        "id": "HsbVCBsOfWGX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "deMRfm9EI40N"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mADmVnL_I40N"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The grid of nine subplots provides a visual representation of images from the training dataset and their respective class labels. This arrangement allows for easy comparison and analysis of the various image classes within the dataset."
      ],
      "metadata": {
        "id": "h978foFVgY4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Preprocessing and Normalizing the Training Dataset\n",
        "- Set the value of **AUTOTUNE** to **tf.data.AUTOTUNE**\n",
        "- Cache the **train_ds** dataset for improved performance by calling the **cache()** method\n",
        "- Shuffle the elements of the **train_ds** dataset using a buffer size of **1000** by calling the **shuffle()** method\n",
        "- Prefetch the elements of the **train_ds** dataset for improved performance by calling the **prefetch()** method with **buffer_size=AUTOTUNE**\n",
        "- Cache the **val_ds** dataset for improved performance by calling the **cache()** method\n",
        "- Prefetch the elements of the **val_ds** dataset for improved performance by calling the **prefetch()** method with **buffer_size=AUTOTUNE**\n",
        "- Create a __Rescaling__ layer to normalize the pixel values of the dataset images to the range **[0, 1]**\n",
        "- Apply the __normalization_layer__ to the **train_ds** dataset using the **map()** method and **lambda** function\n",
        "- Retrieve a batch of images and labels from the normalized dataset using next**(iter(normalized_ds))**\n",
        "- Access the first image in the batch using **image_batch[0]**\n",
        "- Print the minimum and maximum pixel values of the first image using **np.min(first_image)** and __np.max(first_image)__\n"
      ],
      "metadata": {
        "id": "4b8CneoJhGM0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlH9Nw0yI40N"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcJtr9feI40N"
      },
      "outputs": [],
      "source": [
        "normalization_layer = layers.Rescaling(1./255)\n",
        "\n",
        "normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\n",
        "image_batch, labels_batch = next(iter(normalized_ds))\n",
        "first_image = image_batch[0]\n",
        "print(np.min(first_image), np.max(first_image))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The range of pixel values in the normalized dataset ensures that the minimum value is 0 and the maximum value is 1 for the first image. This normalization process allows for consistent and standardized pixel values, facilitating easier comparisons and computations in subsequent analysis."
      ],
      "metadata": {
        "id": "lVamxK_di_f5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 7: Creating a Convolutional Neural Network Model With Data Augmentation\n",
        "- Create a data augmentation pipeline using **keras.Sequential** with three augmentation layers: random horizontal flip, random rotation, and random zoom\n",
        "- Determine the number of classes based on the **class_names**"
      ],
      "metadata": {
        "id": "1vkIInyISxrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXaet5a7I40N"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(img_height,\n",
        "                                  img_width,\n",
        "                                  3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create a sequential model with data augmentation as the first layer and rescaling layer.\n",
        "- Add convolutional layers with an increasing number of filters, 3x3 kernel, padding, and ReLU activation.\n",
        "- Add max pooling layers after each convolutional layer.\n",
        "- Add a dropout layer with a rate of **0.2**.\n",
        "- Add dense layers with ReLU activation, ending with a dense output layer."
      ],
      "metadata": {
        "id": "wswllfMSZGqo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V9_K45oI40O"
      },
      "outputs": [],
      "source": [
        "num_classes = len(class_names)\n",
        "model = Sequential([\n",
        "  data_augmentation,\n",
        "  layers.Rescaling(1./255),\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The output of this code is a convolutional neural network model with data augmentation, suitable for image classification tasks."
      ],
      "metadata": {
        "id": "y7oeyLljbLKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 8: Summarizing and Compiling the Model\n",
        "- Display the summary of the model architecture and the number of parameters\n"
      ],
      "metadata": {
        "id": "rpB-Pkx2cHnv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TB3gDQZoI40O"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Compile the model with the specified optimizer, loss function, and metrics."
      ],
      "metadata": {
        "id": "Gva6uCp_cQf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nnpfibPI40O"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The summary of the model architecture and the compiled model can be seen as output."
      ],
      "metadata": {
        "id": "tJBcd-ySdZJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 9: Training the Model\n",
        "- Set the number of epochs to 3\n",
        "- Train the model using the fit method, passing the training and validation datasets, and the number of epochs"
      ],
      "metadata": {
        "id": "brRhLh-edodU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dfBFEN4ZI40O"
      },
      "outputs": [],
      "source": [
        "epochs=3\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The training history object provides valuable information about the training process, including the loss and accuracy values for each epoch. This object allows us to track the performance of the model over time and analyze how the loss and accuracy metrics evolve during training."
      ],
      "metadata": {
        "id": "kssvH4DBfutz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 10: Visualizing the Result\n",
        "- Retrieve the accuracy and loss values from the training history\n",
        "- Create a range of epochs\n",
        "- Plot the training and validation accuracy in a subplot\n",
        "- Plot the training and validation loss in a subplot\n",
        "- Display the plotted figures"
      ],
      "metadata": {
        "id": "MXJ_XrobfgvO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21SJJw7XI40O"
      },
      "outputs": [],
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- Two subplots displaying the training and validation accuracy, and the training and validation loss over the range of epochs."
      ],
      "metadata": {
        "id": "uAtqqLzfgW6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 11: Predicting the Class of a Given Image\n",
        "- Define the URL of the image and download it using **tf.keras.utils.get_file()**\n",
        "- Load the image and resize it to the desired target size\n",
        "- Convert the image to an array and expand its dimensions to create a batch\n",
        "- Make predictions on the image using the trained model\n",
        "- Calculate the softmax scores and identify the class with the highest confidence\n",
        "- Print the predicted class and its corresponding confidence percentage"
      ],
      "metadata": {
        "id": "grxy1Y1hhh2V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjOCd_QmI40O"
      },
      "outputs": [],
      "source": [
        "\n",
        "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
        "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    sunflower_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Observation:__\n",
        "- The predicted class of the image and the confidence percentage are shown as an output."
      ],
      "metadata": {
        "id": "vV52SPQQiLSX"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}