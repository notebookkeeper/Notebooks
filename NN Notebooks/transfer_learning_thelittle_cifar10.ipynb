{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NHbTb_pXjhVv"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import cv2\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","from tensorflow.keras import datasets"]},{"cell_type":"markdown","metadata":{"id":"-_DJChGHjhVw"},"source":["# Load Your Own Cifar 10 model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"95s1k0PXjhVx","outputId":"d5db97bb-e69e-4e17-bc14-b113a8e309b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 15, 15, 32)        0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 13, 13, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 4, 4, 64)          36928     \n","_________________________________________________________________\n","flatten (Flatten)            (None, 1024)              0         \n","_________________________________________________________________\n","dense (Dense)                (None, 64)                65600     \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 10)                650       \n","=================================================================\n","Total params: 122,570\n","Trainable params: 122,570\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["loaded_model = tf.keras.models.load_model('cifar10.h5')\n","loaded_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BTgASwbkjhVy","outputId":"622dfbe2-e97f-4684-ae41-6c4c82194495"},"outputs":[{"data":{"text/plain":["[<keras.layers.convolutional.Conv2D at 0x7f96e9da16a0>,\n"," <keras.layers.pooling.MaxPooling2D at 0x7f96e9d57e20>,\n"," <keras.layers.convolutional.Conv2D at 0x7f96e864e100>,\n"," <keras.layers.pooling.MaxPooling2D at 0x7f96e9d4b700>,\n"," <keras.layers.convolutional.Conv2D at 0x7f96e9d3f070>,\n"," <keras.layers.core.Flatten at 0x7f96e9d3fdc0>,\n"," <keras.layers.core.Dense at 0x7f96e9dabee0>,\n"," <keras.layers.core.Dense at 0x7f96e9d3f040>]"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["loaded_model.layers"]},{"cell_type":"markdown","metadata":{"id":"bfmvm-wCjhVy"},"source":["# Lets only take layers till the last CNN block from this model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R1mF3lfdjhVz","outputId":"a78cacd5-9e4b-42bf-adfb-93fed58b7abd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_12 (InputLayer)        [(None, 32, 32, 3)]       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 15, 15, 32)        0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 13, 13, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 4, 4, 64)          36928     \n","=================================================================\n","Total params: 56,320\n","Trainable params: 56,320\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["inputs = keras.Input((32,32,3))\n","x = inputs\n","for layer in loaded_model.layers[0:-3]:\n","  x = loaded_model.get_layer(layer.name)(x)\n","\n","custom_model = keras.Model(inputs,x)\n","custom_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dfVMNRqfjhVz"},"outputs":[],"source":["# lets check if there is weights\n","custom_model.layers[1].get_weights()\n","\n","# there is indeed weights"]},{"cell_type":"markdown","metadata":{"id":"GAwUZsJejhVz"},"source":["# Lets load our cats and dogs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d_B-ZIHjhVz","outputId":"9fc75204-88f7-4967-bcbe-545649b97fee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 40 images belonging to 2 classes.\n","Found 20 images belonging to 2 classes.\n","{'cats': 0, 'dogs': 1}\n"]}],"source":["import pathlib\n","data_dir = pathlib.Path('../data/dogs_vs_cats/')\n","\n","train_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    rescale=1.0/255.)\n","\n","test_gen = tf.keras.preprocessing.image.ImageDataGenerator(\n","    rotation_range=20,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    rescale=1.0/255.)\n","\n","batch_size = 32\n","img_height = 32\n","img_width = 32\n","train_ds = train_gen.flow_from_directory(\n","  str(data_dir) + '/train',\n","  seed=123,\n","  target_size=(img_height, img_width),\n","  batch_size=batch_size)\n","val_ds = train_gen.flow_from_directory(\n","  str(data_dir) + '/test',\n","  seed=123,\n","  target_size=(img_height, img_width),\n","  batch_size=batch_size)\n","\n","class_names = train_ds.class_indices\n","print(class_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKzq754yjhV0","outputId":"91493771-caf2-4b86-a340-b7a86f1405da"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model_10\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_20 (InputLayer)        [(None, 32, 32, 3)]       0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 30, 30, 32)        896       \n","_________________________________________________________________\n","max_pooling2d_4 (MaxPooling2 (None, 15, 15, 32)        0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 13, 13, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_5 (MaxPooling2 (None, 6, 6, 64)          0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 4, 4, 64)          36928     \n","_________________________________________________________________\n","flatten_7 (Flatten)          (None, 1024)              0         \n","_________________________________________________________________\n","dense_14 (Dense)             (None, 128)               131200    \n","_________________________________________________________________\n","dense_15 (Dense)             (None, 2)                 258       \n","=================================================================\n","Total params: 187,778\n","Trainable params: 131,458\n","Non-trainable params: 56,320\n","_________________________________________________________________\n"]}],"source":["### lets add our very own customizations\n","## shape of 180,180\n","## flatten, dense of 128, last layer softmax\n","\n","inputs = keras.Input((32,32,3))\n","x = inputs\n","\n","# freeze all layers of the loaded model\n","# BAM !!!\n","loaded_model.trainable = False\n","\n","for layer in loaded_model.layers[0:-3]:\n","  x = loaded_model.get_layer(layer.name)(x)\n","\n","x = keras.layers.Flatten()(x)\n","x = keras.layers.Dense(128, activation='relu')(x)\n","x = keras.layers.Dense(len(class_names))(x)\n","custom_model = keras.Model(inputs,x)\n","\n","custom_model.compile(optimizer='adam',\n","              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n","              metrics=['accuracy'])\n","\n","custom_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMWySkhMjhV0","outputId":"d73f3777-c716-49b4-ba17-aee2fee99b33"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","2/2 [==============================] - 1s 511ms/step - loss: 0.8767 - accuracy: 0.5000 - val_loss: 0.7703 - val_accuracy: 0.5000\n","Epoch 2/10\n","2/2 [==============================] - 0s 167ms/step - loss: 0.7512 - accuracy: 0.5000 - val_loss: 0.6340 - val_accuracy: 0.6500\n","Epoch 3/10\n","2/2 [==============================] - 0s 293ms/step - loss: 0.7083 - accuracy: 0.5250 - val_loss: 0.7010 - val_accuracy: 0.6000\n","Epoch 4/10\n","2/2 [==============================] - 0s 303ms/step - loss: 0.6769 - accuracy: 0.5500 - val_loss: 0.7388 - val_accuracy: 0.5500\n","Epoch 5/10\n","2/2 [==============================] - 0s 186ms/step - loss: 0.6573 - accuracy: 0.5250 - val_loss: 0.6909 - val_accuracy: 0.5500\n","Epoch 6/10\n","2/2 [==============================] - 1s 435ms/step - loss: 0.6435 - accuracy: 0.7250 - val_loss: 0.7449 - val_accuracy: 0.4000\n","Epoch 7/10\n","2/2 [==============================] - 1s 630ms/step - loss: 0.5721 - accuracy: 0.7750 - val_loss: 0.7482 - val_accuracy: 0.4500\n","Epoch 8/10\n","2/2 [==============================] - 1s 446ms/step - loss: 0.5548 - accuracy: 0.7250 - val_loss: 0.6440 - val_accuracy: 0.6000\n","Epoch 9/10\n","2/2 [==============================] - 0s 229ms/step - loss: 0.6005 - accuracy: 0.7000 - val_loss: 0.7732 - val_accuracy: 0.5500\n","Epoch 10/10\n","2/2 [==============================] - 1s 652ms/step - loss: 0.4924 - accuracy: 0.8250 - val_loss: 0.7636 - val_accuracy: 0.5000\n"]}],"source":["epochs=10\n","history = custom_model.fit(\n","  train_ds,\n","  validation_data=val_ds,\n","  epochs=epochs\n",")"]},{"cell_type":"markdown","metadata":{"id":"KWBnAakcjhV1"},"source":["# What just happened ?? We reused an existing model, and added our customization !\n","# the existing model preserves its weights and biases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l-DVGx9ljhV1"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"590a023392bfbd6bf25d040f547d42ca7bfa1d7d06eb05be9ee62c62a8e24a8d"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}