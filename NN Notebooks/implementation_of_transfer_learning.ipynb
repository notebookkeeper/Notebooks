{"cells":[{"cell_type":"markdown","source":["## __Transfer Learning__\n","- Transfer learning refers to a technique in machine learning where a pre-trained model, typically trained on a large dataset, is used as a starting point for solving a different but related task.\n","- It involves using models trained on one problem as a starting point for a related problem.\n","- It is flexible, allowing the use of pre-trained models directly, as feature extraction preprocessing, and integrated into entirely new models.\n","\n"],"metadata":{"id":"15F8jGyUdtrG"}},{"cell_type":"markdown","source":["## Steps to Be Followed:\n","1. Importing the required libraries\n","2. Adding classifier layers\n","3. Preprocessing and feature extraction"],"metadata":{"id":"LgCmoesorvHh"}},{"cell_type":"markdown","source":["### Step 1: Importing the Required Libraries\n","\n","- The **from tensorflow.keras.utils import load_img** is used to load an image file from the file system.\n","\n","- The **from tensorflow.keras.utils import img_to_array** is used to convert an image loaded with load_img into a NumPy array.\n","\n","- The **from keras.applications.vgg16 import preprocess_input** is used to preprocess the input image array before feeding it to the VGG16 model. VGG16 expects the input images to be preprocessed in a specific way.\n","\n","- The **from keras.applications.vgg16 import VGG16** is used to import the VGG16 model architecture. VGG16 is a popular convolutional neural network model pre-trained on the ImageNet dataset for image classification."],"metadata":{"id":"cKMViz1Ngp8Y"}},{"cell_type":"code","execution_count":2,"source":["from tensorflow.keras.utils import load_img\n","from tensorflow.keras.utils import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","\n","from keras.applications.vgg16 import VGG16\n"],"outputs":[],"metadata":{"id":"5woHmJrJdtrK","executionInfo":{"status":"ok","timestamp":1712941690427,"user_tz":-330,"elapsed":515,"user":{"displayName":"Sandipan Basu","userId":"16690901944599954370"}}}},{"cell_type":"markdown","source":["### Step 2: Adding Classifier Layers\n","- It demonstrates how to load a pre-trained VGG16 model without its classifier layers and then add new custom classifier layers on top of it.\n","- The new model is defined by connecting the output of the pre-trained VGG16 model to a flatten layer, followed by a dense layer with 1024 units and ReLU activation, and finally a dense layer with 10 units and softmax activation for multi-class classification.\n","- The model summary provides an overview of the architecture and layer configurations."],"metadata":{"id":"K_860Ql8kwv-"}},{"cell_type":"code","execution_count":4,"source":["\n","from keras.models import Model\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","\n","# no of classes = 10\n","\n","model = VGG16(include_top=False, input_shape=(300, 300, 3))\n","flat1 = Flatten()(model.layers[-1].output)\n","class1 = Dense(1024, activation='relu')(flat1)\n","output = Dense(10, activation='softmax')(class1)\n","\n","model = Model(inputs=model.inputs, outputs=output)\n","model.summary()"],"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 300, 300, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 300, 300, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 300, 300, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 150, 150, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 150, 150, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 150, 150, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 75, 75, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 75, 75, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 75, 75, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 75, 75, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 37, 37, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 37, 37, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 37, 37, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 37, 37, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 18, 18, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n","                                                                 \n"," flatten_1 (Flatten)         (None, 41472)             0         \n","                                                                 \n"," dense_2 (Dense)             (None, 1024)              42468352  \n","                                                                 \n"," dense_3 (Dense)             (None, 10)                10250     \n","                                                                 \n","=================================================================\n","Total params: 57193290 (218.18 MB)\n","Trainable params: 57193290 (218.18 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h95JEG-VdtrM","outputId":"3f4e9b2a-7ca3-4629-dbfa-e87642471cf5","executionInfo":{"status":"ok","timestamp":1712941793531,"user_tz":-330,"elapsed":1646,"user":{"displayName":"Sandipan Basu","userId":"16690901944599954370"}}}},{"cell_type":"markdown","source":["**Observation**\n","- Running the example defines the new model ready for training and summarizes the model architecture.\n","- We have flattened the output of the last pooling layer and added our new fully connected layers.\n","-The weights of the VGG16 model and the weights for the new model will all be trained together on the new dataset."],"metadata":{"id":"E3tLfn7WdtrN"}},{"cell_type":"markdown","source":["### Step 3: Preprocessing and Feature Extraction\n","- The image is loaded from a file and preprocessed to meet the input requirements of the VGG16 model (resizing, converting to a numpy array, and reshaping).\n","\n","- The modified model is used to predict and extract features from the input image, resulting in a feature vector with a specific shape."],"metadata":{"id":"3DWAS8owlYbP"}},{"cell_type":"code","execution_count":null,"source":["image = load_img('dog.jpg', target_size=(224, 224))\n","image = img_to_array(image)\n","image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","image = preprocess_input(image)\n","model = VGG16()\n","model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","features = model.predict(image)\n","print(features.shape)"],"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n","553467096/553467096 [==============================] - 5s 0us/step\n","1/1 [==============================] - 1s 722ms/step\n","(1, 4096)\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pn9hQjZCdtrN","outputId":"e1f032bc-a048-493d-dba5-8d9dd2a65205"}},{"cell_type":"markdown","source":["**Observation**\n","\n","- The VGG16 model weights are downloaded and loaded successfully, and the extracted features from the input image have a shape of (1, 4096)."],"metadata":{"id":"Jwvk_m95mR7D"}},{"cell_type":"code","execution_count":null,"source":[],"outputs":[],"metadata":{"id":"HfLnQBrHdtrO"}}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}